{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyOZI6sHZnzTw2w7C69uWX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PawelJakubczyk/ml_data_preprocessing_utilities/blob/main/ml_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup: Install Required Libraries"
      ],
      "metadata": {
        "id": "JAvKwxg5WLfH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L07i7KLS_en",
        "outputId": "d49d5221-8e41-47e7-f166-720298527a68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2024.2.2)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "Collecting textaugment\n",
            "  Downloading textaugment-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n",
            "Requirement already satisfied: gensim>=4.0 in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.25.2)\n",
            "Collecting googletrans>=2 (from textaugment)\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0->textaugment) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0->textaugment) (6.4.0)\n",
            "Collecting httpx==0.13.3 (from googletrans>=2->textaugment)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans>=2->textaugment) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hstspreload-2024.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans>=2->textaugment) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.66.1)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=1f626e81dc1719000c82711bb1833d89bdce61de2122a581a6654ecb306e7460\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans, textaugment\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.2.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 textaugment-2.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install language_tool_python\n",
        "%pip install textaugment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Libraries"
      ],
      "metadata": {
        "id": "kL9pJgOuUJ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read manage data\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Data Preprocesing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from re import sub, findall\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "from language_tool_python import LanguageTool\n",
        "from collections import Counter\n",
        "\n",
        "# data augmentation\n",
        "from textaugment import Wordnet"
      ],
      "metadata": {
        "id": "A7_OiMjhUKTJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Functions"
      ],
      "metadata": {
        "id": "FgnPaM7SUNi-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **normalize_text(input_text: str) -> str**:\n",
        "   This function normalizes the input text by converting it to lowercase and removing leading/trailing whitespaces.\n",
        "\n",
        "2. **remove_punctuation(input_string: str) -> str**:\n",
        "   This function removes punctuation marks from the input string.\n",
        "\n",
        "3. **replace_tabs_enters_and_spaces(input_string:str) -> str**:\n",
        "   This function takes an input string and replaces tabs with a single space and multiple consecutive spaces with a single space.\n",
        "\n",
        "4. **remove_stopwords(input_string: str) -> str**:\n",
        "   This function removes stopwords from the input string. Stopwords are commonly used words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning.\n",
        "\n",
        "5. **remove_html_tags(input_text: str) -> str**:\n",
        "   This function removes HTML tags from the input text.\n",
        "\n",
        "6. **remove_stopwords(input_string: str, stop_words: Set[str]) -> str**:\n",
        "   This function removes stopwords from the input string. Stopwords are commonly used words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning.\n",
        "\n",
        "7. **lemmatize(text: str) -> str**:\n",
        "   This function lemmatizes the input text. Lemmatization is similar to stemming but considers the context of the word to determine its base or dictionary form.\n",
        "\n",
        "8. **stemming(text: str) -> str**:\n",
        "   This function performs stemming on the input text. Stemming reduces words to their root or base form, often by removing suffixes.\n",
        "\n",
        "    **Note:** It is recommended to avoid using lemmatization (`lemmatize`) and stemming (`stemming`) simultaneously, as both methods serve to normalize words but in different ways. Using them together may lead to excessive text processing and loss of semantic information.\n",
        "\n",
        "9. **remove_pos_tags(text: str) -> str**:\n",
        "   This function removes all words from the input text except for nouns (NN). It relies on part-of-speech tagging to identify and retain only nouns in the text.\n",
        "\n",
        "10. **correct_grammar(text: str) -> str**:\n",
        "    This function corrects grammar mistakes in the input text.\n",
        "\n",
        "11. **remove_common_words(text: str, common_threshold: int) -> str**:\n",
        "    This function removes words from the text that occur too frequently.\n",
        "\n",
        "12. **remove_rare_words(text: str, rare_threshold: int) -> str**:\n",
        "    This function removes words from the text that occur too rarely.\n",
        "\n",
        "13. **clean_empty_data(df: pd.DataFrame, columns_to_check: list) -> pd.DataFrame**:\n",
        "    Cleans the input DataFrame by removing rows with missing value or empty strings.\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu0eds94WXJN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prior to running the functions, required resources need to be downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text(input_text: str) -> str:\n",
        "    \"\"\"Normalizes the input text by converting it to lowercase and removing leading/trailing whitespaces.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    normalized_text = input_text.lower()\n",
        "\n",
        "    # Remove leading and trailing whitespaces\n",
        "    normalized_text = normalized_text.strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "def remove_punctuation(input_string: str) -> str:\n",
        "    \"\"\"Removes punctuation from the input string\"\"\"\n",
        "    return input_string.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def replace_tabs_enters_and_spaces(input_string:str) -> str:\n",
        "    \"\"\"Replaces tabs with a single space and multiple spaces with a single space in the input string\"\"\"\n",
        "    replaced_tabs = sub(r'\\t', ' ', input_string)\n",
        "    replace_enters = sub(r'\\n', ' ', replaced_tabs)\n",
        "    replaced_spaces = sub(r'\\s+', ' ', replace_enters)\n",
        "    return replaced_spaces\n",
        "\n",
        "def remove_html_tags(input_text: str) -> str:\n",
        "    \"\"\"Removes HTML tags from the input text.\"\"\"\n",
        "    clean_text = sub(r'<.*?>', '', input_text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_stopwords(input_string: str) -> str:\n",
        "    \"\"\"Removes stopwords from the input string\"\"\"\n",
        "    words = input_string.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in STOP_WORDS]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def lemmatize(input_string: str)-> str:\n",
        "    \"\"\"Lemmatizes the input text\"\"\"\n",
        "    doc = nlp(input_string)\n",
        "    sent = [token.lemma_ for token in doc if token.text not in STOP_WORDS]\n",
        "    return ' '.join(sent)\n",
        "\n",
        "def stemming(input_string: str)-> str:\n",
        "    \"\"\"Performs stemming on the input text\"\"\"\n",
        "    tokens = word_tokenize(input_string)\n",
        "    stem_words = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stem_words)\n",
        "\n",
        "def remove_pos_tags(input_string: str)-> str:\n",
        "    \"\"\"Removes all words except nouns (NN) from the input text\"\"\"\n",
        "    doc = nlp(input_string)\n",
        "    sent = [token.text for token in doc if token.tag_ == 'NN']\n",
        "    return ' '.join(sent)\n",
        "\n",
        "def correct_grammar(text: str) -> str:\n",
        "    \"\"\"Corrects grammar mistakes in the input text.\"\"\"\n",
        "    # Creates a LanguageTool object for English\n",
        "    tool = LanguageTool('en-US')\n",
        "    corrected_text = tool.correct(text)\n",
        "    return corrected_text\n",
        "\n",
        "def remove_common_words(text: str, common_threshold: int) -> str:\n",
        "    \"\"\"Removes words from the text that occur too frequently.\"\"\"\n",
        "    words = findall(r'\\b\\w+\\b', text.lower())  # Tokenizacja tekstu na słowa\n",
        "    word_counts = Counter(words)\n",
        "    common_words = {word for word, count in word_counts.items() if count > common_threshold}\n",
        "    filtered_text = ' '.join(word for word in words if word not in common_words)\n",
        "    return filtered_text\n",
        "\n",
        "def remove_rare_words(text: str, rare_threshold: int) -> str:\n",
        "    \"\"\"Removes words from the text that occur too rarely.\"\"\"\n",
        "    words = findall(r'\\b\\w+\\b', text.lower())  # Tokenizacja tekstu na słowa\n",
        "    word_counts = Counter(words)\n",
        "    rare_words = {word for word, count in word_counts.items() if count <= rare_threshold}\n",
        "    filtered_text = ' '.join(word for word in words if word not in rare_words)\n",
        "    return filtered_text\n",
        "\n",
        "def clean_empty_data(df: pd.DataFrame, columns_to_check: list) -> pd.DataFrame:\n",
        "    \"\"\"Cleans the input DataFrame by removing rows with incorrect or invalid values.\"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame\n",
        "    cleaned_df = df.copy()\n",
        "\n",
        "    for column in columns_to_check:\n",
        "        cleaned_df = cleaned_df[cleaned_df[column].notna()]\n",
        "        cleaned_df = cleaned_df[cleaned_df[column] != \"\"]\n",
        "\n",
        "    return cleaned_df"
      ],
      "metadata": {
        "id": "axJjRK-0ULN6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad6d00ec-0034-4a3c-8369-d8cdeb9b9b4f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Usage Examples"
      ],
      "metadata": {
        "id": "KlU5AaK4Yi-v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. **normalize_text(input_text: str) -> str**:\n",
        "   This function normalizes the input text by converting it to lowercase and removing leading/trailing whitespaces."
      ],
      "metadata": {
        "id": "oIhUee4HXDCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Simple text\n",
        "text1 = \"   This is a Sample Text   \"\n",
        "normalized_text1 = normalize_text(text1)\n",
        "print(\"Original Text 1:\", text1)\n",
        "print(\"Normalized Text 1:\", normalized_text1)\n",
        "\n",
        "# Example 2: Text with uppercase letters\n",
        "text2 = \"HELLO WORLD\"\n",
        "normalized_text2 = normalize_text(text2)\n",
        "print(\"Original Text 2:\", text2)\n",
        "print(\"Normalized Text 2:\", normalized_text2)\n",
        "\n",
        "# Example 3: Text with multiple white spaces\n",
        "text3 = \"   This     is     another    example    \"\n",
        "normalized_text3 = normalize_text(text3)\n",
        "print(\"Original Text 3:\", text3)\n",
        "print(\"Normalized Text 3:\", normalized_text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuOoZuPWXDTS",
        "outputId": "7db5b360-b5e1-41db-8ec1-5e86c0453acd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text 1:    This is a Sample Text   \n",
            "Normalized Text 1: this is a sample text\n",
            "Original Text 2: HELLO WORLD\n",
            "Normalized Text 2: hello world\n",
            "Original Text 3:    This     is     another    example    \n",
            "Normalized Text 3: this     is     another    example\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. **remove_punctuation(input_string: str) -> str**:\n",
        "   This function removes punctuation marks from the input string."
      ],
      "metadata": {
        "id": "dZC7ZUxmYeES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1: Text with punctuation\n",
        "text1 = \"Hello, World!\"\n",
        "cleaned_text1 = remove_punctuation(text1)\n",
        "print(\"Original Text 1:\", text1)\n",
        "print(\"Cleaned Text 1:\", cleaned_text1)\n",
        "\n",
        "# Example 2: Text with special characters\n",
        "text2 = \"This is a #test with @special characters!\"\n",
        "cleaned_text2 = remove_punctuation(text2)\n",
        "print(\"Original Text 2:\", text2)\n",
        "print(\"Cleaned Text 2:\", cleaned_text2)\n",
        "\n",
        "# Example 3: Mixed text with punctuation and numbers\n",
        "text3 = \"Don't worry, it's just 9:00 AM!\"\n",
        "cleaned_text3 = remove_punctuation(text3)\n",
        "print(\"Original Text 3:\", text3)\n",
        "print(\"Cleaned Text 3:\", cleaned_text3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RgeAksMTYeZF",
        "outputId": "e20741f2-efd7-4dd9-d02d-9678bba28e48"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text 1: Hello, World!\n",
            "Cleaned Text 1: Hello World\n",
            "Original Text 2: This is a #test with @special characters!\n",
            "Cleaned Text 2: This is a test with special characters\n",
            "Original Text 3: Don't worry, it's just 9:00 AM!\n",
            "Cleaned Text 3: Dont worry its just 900 AM\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. **replace_tabs_enters_and_spaces(input_string:str) -> str**:\n",
        "   This function takes an input string and replaces tabs with a single space and multiple consecutive spaces with a single space."
      ],
      "metadata": {
        "id": "YTFUEDJJZpH5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"This\\tis\\na\\tsample\\ttext\\nwith\\ttabs\\nand\\nnew\\tlines.\"\n",
        "print(\"Original Text:\")\n",
        "print(input_text)\n",
        "\n",
        "processed_text = replace_tabs_enters_and_spaces(input_text)\n",
        "print(\"\\nProcessed Text:\")\n",
        "print(processed_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YV37-NlQZB3C",
        "outputId": "54a408f5-3e45-43e5-a270-5ff0bd3c1255"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Text:\n",
            "This\tis\n",
            "a\tsample\ttext\n",
            "with\ttabs\n",
            "and\n",
            "new\tlines.\n",
            "\n",
            "Processed Text:\n",
            "This is a sample text with tabs and new lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. **remove_stopwords(input_string: str) -> str**:\n",
        "   This function removes stopwords from the input string. Stopwords are commonly used words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning."
      ],
      "metadata": {
        "id": "3V_yx4AZZKS8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "html_text = \"<p>This is <b>HTML</b> <i>text</i> with <a href='#'>links</a>.</p>\"\n",
        "cleaned_text = remove_html_tags(html_text)\n",
        "print(\"Original HTML Text:\")\n",
        "print(html_text)\n",
        "print(\"\\nCleaned Text:\")\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3xu1k-rZ9yX",
        "outputId": "62a631a1-1574-47d3-848c-4994126a3209"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original HTML Text:\n",
            "<p>This is <b>HTML</b> <i>text</i> with <a href='#'>links</a>.</p>\n",
            "\n",
            "Cleaned Text:\n",
            "This is HTML text with links.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. **remove_html_tags(input_text: str) -> str**:\n",
        "   This function removes HTML tags from the input text.\n"
      ],
      "metadata": {
        "id": "ycV7DXKJZOIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "c7DkOxuUZ-MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. **remove_stopwords(input_string: str, stop_words: Set[str]) -> str**:\n",
        "   This function removes stopwords from the input string. Stopwords are commonly used words (e.g., \"the\", \"is\", \"in\") that do not carry significant meaning."
      ],
      "metadata": {
        "id": "-y_3CXk3ZOGI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3iZ9nkCxZ-vG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. **lemmatize(text: str) -> str**:\n",
        "   This function lemmatizes the input text. Lemmatization is similar to stemming but considers the context of the word to determine its base or dictionary form."
      ],
      "metadata": {
        "id": "veuF_T7PZODs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O8Iuwg5qZ_RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. **stemming(text: str) -> str**:\n",
        "   This function performs stemming on the input text. Stemming reduces words to their root or base form, often by removing suffixes.\n",
        "\n",
        "    **Note:** It is recommended to avoid using lemmatization (`lemmatize`) and stemming (`stemming`) simultaneously, as both methods serve to normalize words but in different ways. Using them together may lead to excessive text processing and loss of semantic information."
      ],
      "metadata": {
        "id": "7xOXmfarZOAn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tWtmQ026aADl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. **remove_pos_tags(text: str) -> str**:\n",
        "   This function removes all words from the input text except for nouns (NN). It relies on part-of-speech tagging to identify and retain only nouns in the text.\n"
      ],
      "metadata": {
        "id": "SpPKCfHzZN-E"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BlBcaWJhaAdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. **correct_grammar(text: str) -> str**:\n",
        "    This function corrects grammar mistakes in the input text."
      ],
      "metadata": {
        "id": "k7aDoaPaZNyM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6uksCulfZaka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. **remove_common_words(text: str, common_threshold: int) -> str**:\n",
        "    This function removes words from the text that occur too frequently."
      ],
      "metadata": {
        "id": "7YDbt3ABZbls"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "v7JlnXy6aB5r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 12. **remove_rare_words(text: str, rare_threshold: int) -> str**:\n",
        "    This function removes words from the text that occur too rarely."
      ],
      "metadata": {
        "id": "Dc6FdiR3Zbfq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tb-Re2_-aCRu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 13. **clean_empty_data(df: pd.DataFrame, columns_to_check: list) -> pd.DataFrame**:\n",
        "    Cleans the input DataFrame by removing rows with missing value or empty strings."
      ],
      "metadata": {
        "id": "JCuwVBBeZbda"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ch26s5NuZ8GR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}