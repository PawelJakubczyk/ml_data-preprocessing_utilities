{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNM/GgSVkGGyNV+akoX2pLe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PawelJakubczyk/ml_data_preprocessing_utilities/blob/main/ml_data_preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7L07i7KLS_en",
        "outputId": "5cacc72c-b99c-4264-cbc7-6966a4911c40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
            "Collecting language_tool_python\n",
            "  Downloading language_tool_python-2.7.1-py3-none-any.whl (34 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from language_tool_python) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->language_tool_python) (2024.2.2)\n",
            "Installing collected packages: language_tool_python\n",
            "Successfully installed language_tool_python-2.7.1\n",
            "Collecting textaugment\n",
            "  Downloading textaugment-2.0.0-py3-none-any.whl (19 kB)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from textaugment) (3.8.1)\n",
            "Requirement already satisfied: gensim>=4.0 in /usr/local/lib/python3.10/dist-packages (from textaugment) (4.3.2)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.10/dist-packages (from textaugment) (0.17.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from textaugment) (1.25.2)\n",
            "Collecting googletrans>=2 (from textaugment)\n",
            "  Downloading googletrans-3.0.0.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0->textaugment) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim>=4.0->textaugment) (6.4.0)\n",
            "Collecting httpx==0.13.3 (from googletrans>=2->textaugment)\n",
            "  Downloading httpx-0.13.3-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m1.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans>=2->textaugment) (2024.2.2)\n",
            "Collecting hstspreload (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hstspreload-2024.2.1-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx==0.13.3->googletrans>=2->textaugment) (1.3.0)\n",
            "Collecting chardet==3.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading chardet-3.0.4-py2.py3-none-any.whl (133 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.4/133.4 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna==2.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading idna-2.10-py2.py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.8/58.8 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3 (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading rfc3986-1.5.0-py2.py3-none-any.whl (31 kB)\n",
            "Collecting httpcore==0.9.* (from httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading httpcore-0.9.1-py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.10,>=0.8 (from httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading h11-0.9.0-py2.py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h2==3.* (from httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading h2-3.2.0-py2.py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.0/65.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hyperframe<6,>=5.2.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hyperframe-5.2.0-py2.py3-none-any.whl (12 kB)\n",
            "Collecting hpack<4,>=3.0 (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans>=2->textaugment)\n",
            "  Downloading hpack-3.0.0-py2.py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->textaugment) (4.66.1)\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.0.0-py3-none-any.whl size=15715 sha256=acc9ebafe2fa5c714d574d3cebdbd7b5dc1308ee9badb7391c1cfbcb55db7f5e\n",
            "  Stored in directory: /root/.cache/pip/wheels/b3/81/ea/8b030407f8ebfc2f857814e086bb22ca2d4fea1a7be63652ab\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hyperframe, hpack, h11, chardet, idna, hstspreload, h2, httpcore, httpx, googletrans, textaugment\n",
            "  Attempting uninstall: chardet\n",
            "    Found existing installation: chardet 5.2.0\n",
            "    Uninstalling chardet-5.2.0:\n",
            "      Successfully uninstalled chardet-5.2.0\n",
            "  Attempting uninstall: idna\n",
            "    Found existing installation: idna 3.6\n",
            "    Uninstalling idna-3.6:\n",
            "      Successfully uninstalled idna-3.6\n",
            "Successfully installed chardet-3.0.4 googletrans-3.0.0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2024.2.1 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 idna-2.10 rfc3986-1.5.0 textaugment-2.0.0\n"
          ]
        }
      ],
      "source": [
        "%pip install numpy\n",
        "%pip install pandas\n",
        "%pip install language_tool_python\n",
        "%pip install textaugment"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "kL9pJgOuUJ4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read manage data\n",
        "import pandas as pd\n",
        "from concurrent.futures import ProcessPoolExecutor\n",
        "\n",
        "# Data Preprocesing\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from re import sub, findall\n",
        "import string\n",
        "import nltk\n",
        "import spacy\n",
        "from language_tool_python import LanguageTool\n",
        "from collections import Counter\n",
        "\n",
        "# data augmentation\n",
        "from textaugment import Wordnet"
      ],
      "metadata": {
        "id": "A7_OiMjhUKTJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FgnPaM7SUNi-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prior to running the functions, required resources need to be downloaded\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "STOP_WORDS = nlp.Defaults.stop_words\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def normalize_text(input_text: str) -> str:\n",
        "    \"\"\"Normalizes the input text by converting it to lowercase and removing leading/trailing whitespaces.\"\"\"\n",
        "    # Convert to lowercase\n",
        "    normalized_text = input_text.lower()\n",
        "\n",
        "    # Remove leading and trailing whitespaces\n",
        "    normalized_text = normalized_text.strip()\n",
        "\n",
        "    return normalized_text\n",
        "\n",
        "def remove_punctuation(input_string: str) -> str:\n",
        "    \"\"\"Removes punctuation from the input string\"\"\"\n",
        "    return input_string.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "def replace_tabs_enters_and_spaces(input_string:str) -> str:\n",
        "    \"\"\"Replaces tabs with a single space and multiple spaces with a single space in the input string\"\"\"\n",
        "    replaced_tabs = sub(r'\\t', ' ', input_string)\n",
        "    replace_enters = sub(r'\\n', ' ', replaced_tabs)\n",
        "    replaced_spaces = sub(r'\\s+', ' ', replace_enters)\n",
        "    return replaced_spaces\n",
        "\n",
        "def remove_html_tags(input_text: str) -> str:\n",
        "    \"\"\"Removes HTML tags from the input text.\"\"\"\n",
        "    clean_text = sub(r'<.*?>', '', input_text)\n",
        "    return clean_text\n",
        "\n",
        "def remove_stopwords(input_string: str) -> str:\n",
        "    \"\"\"Removes stopwords from the input string\"\"\"\n",
        "    words = input_string.split()\n",
        "    filtered_words = [word for word in words if word.lower() not in STOP_WORDS]\n",
        "    return ' '.join(filtered_words)\n",
        "\n",
        "def lemmatize(input_string: str)-> str:\n",
        "    \"\"\"Lemmatizes the input text\"\"\"\n",
        "    doc = nlp(input_string)\n",
        "    sent = [token.lemma_ for token in doc if token.text not in STOP_WORDS]\n",
        "    return ' '.join(sent)\n",
        "\n",
        "def stemming(input_string: str)-> str:\n",
        "    \"\"\"Performs stemming on the input text\"\"\"\n",
        "    tokens = word_tokenize(input_string)\n",
        "    stem_words = [stemmer.stem(word) for word in tokens]\n",
        "    return ' '.join(stem_words)\n",
        "\n",
        "def remove_pos_tags(input_string: str)-> str:\n",
        "    \"\"\"Removes all words except nouns (NN) from the input text\"\"\"\n",
        "    doc = nlp(input_string)\n",
        "    sent = [token.text for token in doc if token.tag_ == 'NN']\n",
        "    return ' '.join(sent)\n",
        "\n",
        "\n",
        "def correct_grammar(text: str) -> str:\n",
        "    \"\"\"Corrects grammar mistakes in the input text.\"\"\"\n",
        "    # Creates a LanguageTool object for English\n",
        "    tool = LanguageTool('en-US')\n",
        "    corrected_text = tool.correct(text)\n",
        "    return corrected_text\n",
        "\n",
        "def remove_common_words(text: str, common_threshold: int) -> str:\n",
        "    \"\"\"Removes words from the text that occur too frequently.\"\"\"\n",
        "    words = findall(r'\\b\\w+\\b', text.lower())  # Tokenizacja tekstu na słowa\n",
        "    word_counts = Counter(words)\n",
        "    common_words = {word for word, count in word_counts.items() if count > common_threshold}\n",
        "    filtered_text = ' '.join(word for word in words if word not in common_words)\n",
        "    return filtered_text\n",
        "\n",
        "def remove_rare_words(text: str, rare_threshold: int) -> str:\n",
        "    \"\"\"Removes words from the text that occur too rarely.\"\"\"\n",
        "    words = findall(r'\\b\\w+\\b', text.lower())  # Tokenizacja tekstu na słowa\n",
        "    word_counts = Counter(words)\n",
        "    rare_words = {word for word, count in word_counts.items() if count <= rare_threshold}\n",
        "    filtered_text = ' '.join(word for word in words if word not in rare_words)\n",
        "    return filtered_text\n",
        "\n",
        "def clean_empty_data(df: pd.DataFrame, columns_to_check: list) -> pd.DataFrame:\n",
        "    \"\"\"Cleans the input DataFrame by removing rows with incorrect or invalid values.\"\"\"\n",
        "    # Make a copy to avoid modifying the original DataFrame\n",
        "    cleaned_df = df.copy()\n",
        "\n",
        "    for column in columns_to_check:\n",
        "        cleaned_df = cleaned_df[cleaned_df[column].notna()]\n",
        "        cleaned_df = cleaned_df[cleaned_df[column] != \"\"]\n",
        "\n",
        "    return cleaned_df\n",
        "\n",
        "# Only the first two preprocessing functions are used because they have the highest percentage of coverage in the model.\n",
        "# The remaining functions are left in the notebook to demonstrate various preprocessing options."
      ],
      "metadata": {
        "id": "axJjRK-0ULN6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}